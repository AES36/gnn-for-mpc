import numpy as np
from pandas import read_csv
import tensorflow as tf
import argparse
import os
import random
from sklearn.model_selection import train_test_split
# QPGNNPolicy sÄ±nÄ±fÄ±nÄ±n models.py dosyasÄ±nda tanÄ±mlÄ± olduÄŸunu varsayÄ±yoruz.
# EÄŸer models.py dosyanÄ±z farklÄ± bir konumdaysa veya sÄ±nÄ±f adÄ± farklÄ±ysa,
# bu satÄ±rÄ± kendi projenize gÃ¶re dÃ¼zenlemeniz gerekebilir.
from models import QPGNNPolicy 

# --- ArgÃ¼manlarÄ±n TanÄ±mlanmasÄ± ---
parser = argparse.ArgumentParser(description="QP GNN Model EÄŸitim Scripti")
parser.add_argument("--data_folder", help="EÄŸitim verilerinin bulunduÄŸu klasÃ¶r", default="./train_data", type=str)
parser.add_argument("--total_samples", help="KullanÄ±lacak toplam eÄŸitim Ã¶rneÄŸi sayÄ±sÄ±", default=2000, type=int)
parser.add_argument("--val_split", help="DoÄŸrulama (validation) veri setinin oranÄ±", default=0.2, type=float)
parser.add_argument("--gpu", help="KullanÄ±lacak GPU indeksi (-1 CPU iÃ§in)", default="0", type=str)
parser.add_argument("--emb_size", help="GNN gÃ¶mme (embedding) boyutu", default=32, type=int)
parser.add_argument("--epochs", help="Maksimum epoch (eÄŸitim turu) sayÄ±sÄ±", default=100, type=int)
parser.add_argument("--type", help="Model tÃ¼rÃ¼", default="fea", choices=['fea','obj','sol']) # 'fea': olabilirlik, 'obj': amaÃ§ fonksiyonu, 'sol': Ã§Ã¶zÃ¼m
parser.add_argument("--lr_schedule", help="Ã–ÄŸrenme oranÄ± (Learning Rate) Ã§izelgesi tÃ¼rÃ¼", default="adaptive", choices=['fixed', 'adaptive', 'cosine', 'exponential'])
parser.add_argument("--dropout", help="Dropout oranÄ±", default=0.1, type=float)
parser.add_argument("--weight_decay", help="AÄŸÄ±rlÄ±k dÃ¼ÅŸÃ¼ÅŸÃ¼ (L2 regularizasyon)", default=1e-5, type=float)
parser.add_argument("--verbose", help="DetaylÄ± Ã§Ä±ktÄ± gÃ¶ster", action="store_true") # Bu bir flag argÃ¼manÄ±dÄ±r, belirtilirse True olur
args = parser.parse_args()

# --- Adaptif Ã–ÄŸrenme OranÄ± YÃ¶neticisi ---
class AdaptiveLRManager:
    """
    Model eÄŸitiminde Ã¶ÄŸrenme oranÄ±nÄ± dinamik olarak yÃ¶neten sÄ±nÄ±f.
    FarklÄ± Ã§izelgeler (sabit, adaptif, kosinÃ¼s, Ã¼stel) destekler.
    """
    def __init__(self, optimizer, schedule_type='adaptive', total_epochs=100):
        self.optimizer = optimizer
        self.schedule_type = schedule_type
        self.total_epochs = total_epochs
        
        # Adaptif Ã¶ÄŸrenme oranÄ± iÃ§in parametreler
        self.patience = 10 # DoÄŸrulama kaybÄ±nÄ±n kaÃ§ epoch boyunca kÃ¶tÃ¼leÅŸmesine izin verileceÄŸi
        self.factor = 0.5  # Ã–ÄŸrenme oranÄ±nÄ±n dÃ¼ÅŸÃ¼rÃ¼leceÄŸi Ã§arpan
        self.min_lr = 1e-7 # Ã–ÄŸrenme oranÄ±nÄ±n dÃ¼ÅŸebileceÄŸi minimum deÄŸer
        self.wait = 0      # DoÄŸrulama kaybÄ±nÄ±n kÃ¶tÃ¼leÅŸtiÄŸi ardÄ±ÅŸÄ±k epoch sayÄ±sÄ±
        self.best_loss = float('inf') # Åimdiye kadarki en iyi doÄŸrulama kaybÄ±
        
        # Modelin tÃ¼rÃ¼ne ve parametrelerine gÃ¶re baÅŸlangÄ±Ã§ Ã¶ÄŸrenme oranÄ±nÄ± otomatik belirler
        self.initial_lr = self._get_optimal_lr()
        self.current_lr = self.initial_lr
        optimizer.learning_rate.assign(self.initial_lr) # OptimizatÃ¶rÃ¼n Ã¶ÄŸrenme oranÄ±nÄ± ayarlar
        
        print(f"ğŸ¯ Otomatik seÃ§ilen baÅŸlangÄ±Ã§ Ã¶ÄŸrenme oranÄ±: {self.initial_lr:.2e} (Ã§izelge: {schedule_type})")
        
    def _get_optimal_lr(self):
        """
        Deneyim ve araÅŸtÄ±rmalara dayalÄ± olarak baÅŸlangÄ±Ã§ Ã¶ÄŸrenme oranÄ±nÄ± otomatik olarak belirler.
        Modelin gÃ¶mme boyutu ve tahmin tÃ¼rÃ¼ (obje, Ã§Ã¶zÃ¼m, olabilirlik) dikkate alÄ±nÄ±r.
        """
        base_lr = 0.001 # Temel Ã¶ÄŸrenme oranÄ±
        
        # GÃ¶mme boyutuna gÃ¶re ayarlama: Daha bÃ¼yÃ¼k modeller genellikle daha kÃ¼Ã§Ã¼k LR'lere ihtiyaÃ§ duyar
        emb_factor = min(1.0, 64 / max(args.emb_size, 1))
        
        # Model tÃ¼rÃ¼ne gÃ¶re ayarlama
        if hasattr(args, 'type'):
            if args.type == 'obj':
                base_lr *= 0.5  # AmaÃ§ fonksiyonu tahmini daha hassas Ã¶ÄŸrenme gerektirebilir
            elif args.type == 'sol':
                base_lr *= 0.8  # Ã‡Ã¶zÃ¼m tahmini orta karmaÅŸÄ±klÄ±kta olabilir
            # 'fea' (olabilirlik) modeli iÃ§in temel oran korunur
        
        return base_lr * emb_factor
        
    def _get_lr_for_epoch(self, epoch):
        """Belirli bir epoch iÃ§in Ã¶ÄŸrenme oranÄ±nÄ± dÃ¶ndÃ¼rÃ¼r (adaptif olmayan Ã§izelgeler iÃ§in)."""
        if self.schedule_type == 'cosine':
            # KosinÃ¼s soÄŸutmasÄ± (Cosine annealing): Ã–ÄŸrenme oranÄ±nÄ± kosinÃ¼s fonksiyonuna gÃ¶re yavaÅŸÃ§a dÃ¼ÅŸÃ¼rÃ¼r
            return self.initial_lr * 0.5 * (1 + np.cos(np.pi * epoch / self.total_epochs))
        elif self.schedule_type == 'exponential':
            # Ãœstel dÃ¼ÅŸÃ¼ÅŸ (Exponential decay): Belirli aralÄ±klarla Ã¶ÄŸrenme oranÄ±nÄ± dÃ¼ÅŸÃ¼rÃ¼r
            decay_rate = 0.95
            return self.initial_lr * (decay_rate ** (epoch // 10)) # Her 10 epoch'ta bir dÃ¼ÅŸÃ¼ÅŸ
        else:
            return self.current_lr # Sabit Ã¶ÄŸrenme oranÄ± iÃ§in mevcut deÄŸeri dÃ¶ndÃ¼r
    
    def step(self, epoch, val_loss=None):
        """
        Ã–ÄŸrenme oranÄ±nÄ± mevcut Ã§izelge tÃ¼rÃ¼ne gÃ¶re gÃ¼nceller.
        Adaptif Ã§izelge iÃ§in doÄŸrulama kaybÄ±nÄ± kullanÄ±r.
        """
        lr_changed = False # Ã–ÄŸrenme oranÄ±nÄ±n bu adÄ±mda deÄŸiÅŸip deÄŸiÅŸmediÄŸini izler
        
        if self.schedule_type == 'adaptive':
            if val_loss is not None:
                if val_loss < self.best_loss:
                    self.best_loss = val_loss
                    self.wait = 0 # En iyi kayÄ±p bulundu, bekleme sayacÄ±nÄ± sÄ±fÄ±rla
                else:
                    self.wait += 1 # KayÄ±p iyileÅŸmedi, bekleme sayacÄ±nÄ± artÄ±r
                    
                if self.wait >= self.patience:
                    old_lr = self.current_lr
                    self.current_lr = max(old_lr * self.factor, self.min_lr) # Ã–ÄŸrenme oranÄ±nÄ± dÃ¼ÅŸÃ¼r
                    self.optimizer.learning_rate.assign(self.current_lr) # OptimizatÃ¶rÃ¼ gÃ¼ncelle
                    self.wait = 0 # Bekleme sayacÄ±nÄ± sÄ±fÄ±rla
                    
                    if old_lr != self.current_lr:
                        print(f"  ğŸ“‰ Adaptif LR: {old_lr:.2e} â†’ {self.current_lr:.2e}")
                        lr_changed = True
        
        elif self.schedule_type in ['cosine', 'exponential']:
            new_lr = self._get_lr_for_epoch(epoch)
            if abs(new_lr - self.current_lr) > 1e-8: # Ã–nemli bir deÄŸiÅŸiklik varsa gÃ¼ncelle
                old_lr = self.current_lr
                self.current_lr = new_lr
                self.optimizer.learning_rate.assign(self.current_lr)
                print(f"  ğŸ“‰ Ã‡izelgeli LR: {old_lr:.2e} â†’ {self.current_lr:.2e}")
                lr_changed = True
        
        return lr_changed

# --- QP Model EÄŸitici SÄ±nÄ±fÄ± ---
class QPModelTrainer:
    """
    Kare Programlama (QP) GNN modelinin eÄŸitim sÃ¼recini baÅŸtan sona yÃ¶netir.
    GPU kurulumu, veri yÃ¼kleme, model oluÅŸturma, eÄŸitim ve doÄŸrulama adÄ±mlarÄ±nÄ± iÃ§erir.
    """
    def __init__(self):
        self.train_indices = None
        self.val_indices = None
        self.data_dimensions = {} # YÃ¼klenen GNN verilerinin boyutlarÄ±nÄ± saklar
        self.model = None
        self.optimizer = None
        self.lr_manager = None
        self.obj_mean = None      # AmaÃ§ fonksiyonu normalizasyonu iÃ§in ortalama
        self.obj_std = None       # AmaÃ§ fonksiyonu normalizasyonu iÃ§in standart sapma
        
    def setup_gpu(self):
        """
        TensorFlow iÃ§in GPU yapÄ±landÄ±rmasÄ±nÄ± ayarlar. 
        Belirtilen GPU indeksini kullanÄ±r veya CPU'ya dÃ¼ÅŸer.
        """
        if args.gpu == "-1":
            print("ğŸ–¥ï¸  CPU Ã¼zerinde Ã§alÄ±ÅŸÄ±yor.")
            tf.config.set_visible_devices([], 'GPU') # TÃ¼m GPU'larÄ± devre dÄ±ÅŸÄ± bÄ±rak
            return "/CPU:0"
        else:
            gpu_index = int(args.gpu)
            gpus = tf.config.list_physical_devices('GPU')
            if len(gpus) > 0:
                try:
                    tf.config.set_visible_devices(gpus[gpu_index], 'GPU')
                    tf.config.experimental.set_memory_growth(gpus[gpu_index], True) # GPU bellek bÃ¼yÃ¼mesini etkinleÅŸtir
                    print(f"ğŸš€ GPU {gpu_index} kullanÄ±lÄ±yor: {gpus[gpu_index].name}")
                    return f"/GPU:{gpu_index}"
                except Exception as e:
                    print(f"âš ï¸  GPU kurulumu baÅŸarÄ±sÄ±z: {e}. CPU kullanÄ±lÄ±yor.")
                    return "/CPU:0"
            else:
                print("âŒ GPU bulunamadÄ±. CPU kullanÄ±lÄ±yor.")
                return "/CPU:0"
    
    def detect_data_dimensions(self):
        """
        Veri klasÃ¶rÃ¼ndeki ilk Ã¶rnek dosyayÄ± okuyarak GNN giriÅŸlerinin boyutlarÄ±nÄ± algÄ±lar.
        Bu boyutlar modelin baÅŸlatÄ±lmasÄ± iÃ§in gereklidir.
        """
        sample_dir = os.path.join(args.data_folder, "Data_0", "Data_0") # Ä°lk Ã¶rnek veri dizini
        try:
            # Ã–rnek dosyalarÄ± bir kez okuyarak boyutlarÄ± Ã§Ä±kar
            sample_var = read_csv(os.path.join(sample_dir, "VarFeatures.csv"), header=None)
            sample_con = read_csv(os.path.join(sample_dir, "ConFeatures.csv"), header=None)
            sample_edge_A = read_csv(os.path.join(sample_dir, "EdgeFeatures_A.csv"), header=None)
            sample_qedge_H = read_csv(os.path.join(sample_dir, "QEdgeFeatures.csv"), header=None)
            
            self.data_dimensions = {
                'n_vars': sample_var.shape[0],          # Grafik baÅŸÄ±na deÄŸiÅŸken (dÃ¼ÄŸÃ¼m) sayÄ±sÄ±
                'n_cons': sample_con.shape[0],          # Grafik baÅŸÄ±na kÄ±sÄ±t (dÃ¼ÄŸÃ¼m) sayÄ±sÄ±
                'var_features': sample_var.shape[1],    # DeÄŸiÅŸken dÃ¼ÄŸÃ¼m Ã¶zelliklerinin boyutu
                'con_features': sample_con.shape[1],    # KÄ±sÄ±t dÃ¼ÄŸÃ¼m Ã¶zelliklerinin boyutu
                'edge_features': sample_edge_A.shape[1], # Kenar Ã¶zelliklerinin boyutu (Ax matrisleri iÃ§in)
                'qedge_features': sample_qedge_H.shape[1] # Kare kenar Ã¶zelliklerinin boyutu (Q matrisi iÃ§in)
            }
            
            print(f"ğŸ“Š AlgÄ±lanan veri boyutlarÄ±:")
            print(f"   Grafik baÅŸÄ±na deÄŸiÅŸken: {self.data_dimensions['n_vars']}")
            print(f"   Grafik baÅŸÄ±na kÄ±sÄ±t: {self.data_dimensions['n_cons']}")
            print(f"   DeÄŸiÅŸken Ã¶zellikleri: {self.data_dimensions['var_features']}")
            print(f"   KÄ±sÄ±t Ã¶zellikleri: {self.data_dimensions['con_features']}")
            print(f"   Kenar Ã¶zellikleri: {self.data_dimensions['edge_features']}")
            print(f"   QKenar Ã¶zellikleri: {self.data_dimensions['qedge_features']}")
            
        except FileNotFoundError as e:
            print(f"âŒ Boyut algÄ±lama baÅŸarÄ±sÄ±z: {e}. VarsayÄ±lan boyutlar kullanÄ±lÄ±yor.")
            # Dosya bulunamazsa veya hata oluÅŸursa varsayÄ±lan boyutlarÄ± kullan
            self.data_dimensions = {
                'n_vars': 10, 'n_cons': 40, 'var_features': 3,
                'con_features': 2, 'edge_features': 1, 'qedge_features': 1
            }
    
    def create_train_val_split(self):
        """
        Mevcut veri Ã¶rneklerinden eÄŸitim ve doÄŸrulama veri setlerini oluÅŸturur.
        `'obj'` model tÃ¼rÃ¼ iÃ§in amaÃ§ fonksiyonu etiketlerinin Z-skor istatistiklerini hesaplar.
        """
        available_indices = []
        for i in range(args.total_samples):
            instance_dir = os.path.join(args.data_folder, f"Data_{i}")
            feas_path = os.path.join(instance_dir, "Labels_feas.csv")
            
            if not os.path.exists(feas_path):
                continue # Olabilirlik etiketi yoksa atla
                
            # 'obj' veya 'sol' modelleri iÃ§in yalnÄ±zca mÃ¼mkÃ¼n (feasible) Ã¶rnekleri dahil et
            if args.type in ["obj", "sol"]:
                try:
                    is_feasible = read_csv(feas_path, header=None).values[0,0]
                    if is_feasible == 0:
                        continue  # Olmayan durumlarÄ± atla
                except:
                    continue # Okuma hatasÄ± olursa atla
            
            # SeÃ§ilen model tÃ¼rÃ¼ne gÃ¶re ilgili etiket dosyasÄ±nÄ±n varlÄ±ÄŸÄ±nÄ± kontrol et
            if args.type == "obj":
                if not os.path.exists(os.path.join(instance_dir, "Labels_obj.csv")):
                    continue
            elif args.type == "sol":
                if not os.path.exists(os.path.join(instance_dir, "Labels_solu.csv")):
                    continue
                    
            available_indices.append(i) # GeÃ§erli veri indeksini listeye ekle
        
        if len(available_indices) == 0:
            raise ValueError("Hata: EÄŸitim iÃ§in geÃ§erli veri bulunamadÄ±!")
        
        # scikit-learn'den train_test_split kullanarak eÄŸitim ve doÄŸrulama indekslerini ayÄ±r
        self.train_indices, self.val_indices = train_test_split(
            available_indices,
            test_size=args.val_split, # DoÄŸrulama veri setinin oranÄ±
            random_state=42,          # Tekrarlanabilirlik iÃ§in sabit rastgele durum
            shuffle=True              # Veriyi karÄ±ÅŸtÄ±r
        )

        # EÄŸer model tÃ¼rÃ¼ 'obj' (amaÃ§ fonksiyonu tahmini) ise, normalizasyon iÃ§in istatistikleri hesapla
        if args.type == "obj":
            print("AmaÃ§ fonksiyonu etiketleri iÃ§in Z-skor normalizasyon istatistikleri hesaplanÄ±yor...")
            all_obj_labels = []
            # Sadece eÄŸitim verilerinden istatistikleri topla (veri sÄ±zÄ±ntÄ±sÄ±nÄ± Ã¶nle)
            for i in self.train_indices: 
                instance_dir = os.path.join(args.data_folder, f"Data_{i}")
                try:
                    obj_label = read_csv(os.path.join(instance_dir, "Labels_obj.csv"), header=None).values[0,0]
                    all_obj_labels.append(obj_label)
                except Exception as e:
                    if args.verbose:
                        print(f"âš ï¸ Ä°statistik toplama sÄ±rasÄ±nda Data_{i} iÃ§in amaÃ§ etiketi yÃ¼klenemedi: {e}")
                    continue
            
            if all_obj_labels:
                self.obj_mean = np.mean(all_obj_labels) # OrtalamayÄ± hesapla
                self.obj_std = np.std(all_obj_labels)   # Standart sapmayÄ± hesapla
                # SÄ±fÄ±ra bÃ¶lmeyi Ã¶nlemek iÃ§in, eÄŸer standart sapma sÄ±fÄ±rsa (tÃ¼m deÄŸerler aynÄ±ysa) 1.0 yap
                if self.obj_std == 0:
                    self.obj_std = 1.0 
                print(f"   AmaÃ§ Normalizasyonu Ä°statistikleri: Ortalama={self.obj_mean:.4f}, Std={self.obj_std:.4f}")
            else:
                print("   UyarÄ±: Normalizasyon iÃ§in amaÃ§ etiketi bulunamadÄ±. Normalizasyon atlanÄ±yor.")
                self.obj_mean = 0.0 # VarsayÄ±lan olarak normalizasyon yapma
                self.obj_std = 1.0  # VarsayÄ±lan olarak normalizasyon yapma
        
        print(f"ğŸ“Š Veri ayrÄ±mÄ±:")
        print(f"   Toplam mevcut Ã¶rnek: {len(available_indices)}")
        print(f"   EÄŸitim Ã¶rnekleri: {len(self.train_indices)}")
        print(f"   DoÄŸrulama Ã¶rnekleri: {len(self.val_indices)}")
        print(f"   AyÄ±rma oranÄ±: {args.val_split:.1%}")
    
    def load_batch_data(self, indices):
        """
        Verilen indeksler iÃ§in GNN giriÅŸ verilerini ve etiketlerini yÃ¼kler ve 
        sÃ¼per-grafik formatÄ±nda birleÅŸtirir.
        """
        varFeatures_list = []
        conFeatures_list = []
        edgFeatures_A_list = []
        edgIndices_A_list = []
        q_edgFeatures_H_list = []
        q_edgIndices_H_list = []
        labels_list = []

        var_node_offset = 0 # BirleÅŸik sÃ¼per-grafikte deÄŸiÅŸken dÃ¼ÄŸÃ¼m indekslerinin ofseti
        con_node_offset = 0 # BirleÅŸik sÃ¼per-grafikte kÄ±sÄ±t dÃ¼ÄŸÃ¼m indekslerinin ofseti

        for i in indices:
            instance_dir = os.path.join(args.data_folder, f"Data_{i}")
            gnn_data_dir = os.path.join(instance_dir, f"Data_{i}")
            
            try:
                # Etiketleri model tÃ¼rÃ¼ne gÃ¶re yÃ¼kle
                if args.type == "fea":
                    feas_label = read_csv(os.path.join(instance_dir, "Labels_feas.csv"), header=None).values[0,0]
                    labels_data = np.array([[feas_label]])
                elif args.type == "obj":
                    labels_data = read_csv(os.path.join(instance_dir, "Labels_obj.csv"), header=None).values
                    # AmaÃ§ etiketlerine Z-skor normalizasyonu uygula
                    if self.obj_mean is not None and self.obj_std is not None:
                        labels_data = (labels_data - self.obj_mean) / self.obj_std
                elif args.type == "sol":
                    labels_data = read_csv(os.path.join(instance_dir, "Labels_solu.csv"), header=None).values

                # GNN Ã¶zelliklerini CSV dosyalarÄ±ndan yÃ¼kle
                var_features = read_csv(os.path.join(gnn_data_dir, "VarFeatures.csv"), header=None).values
                con_features = read_csv(os.path.join(gnn_data_dir, "ConFeatures.csv"), header=None).values
                edg_features_A = read_csv(os.path.join(gnn_data_dir, "EdgeFeatures_A.csv"), header=None).values
                edg_indices_A = read_csv(os.path.join(gnn_data_dir, "EdgeIndices_A.csv"), header=None).values
                q_edg_features_H = read_csv(os.path.join(gnn_data_dir, "QEdgeFeatures.csv"), header=None).values
                q_edg_indices_H = read_csv(os.path.join(gnn_data_dir, "QEdgeIndices.csv"), header=None).values

                # Birden fazla grafiÄŸi tek bir sÃ¼per-grafikte birleÅŸtirmek iÃ§in indeks ofsetlerini uygula
                # KÄ±sÄ±t-deÄŸiÅŸken kenarlarÄ± iÃ§in ofset
                edg_indices_A_offset = edg_indices_A + [con_node_offset, var_node_offset]
                # DeÄŸiÅŸken-deÄŸiÅŸken kenarlarÄ± (kuadratik) iÃ§in ofset
                q_edg_indices_H_offset = q_edg_indices_H + [var_node_offset, var_node_offset]

                # TÃ¼m yÃ¼klenen verileri listelere ekle
                varFeatures_list.append(var_features)
                conFeatures_list.append(con_features)
                edgFeatures_A_list.append(edg_features_A)
                edgIndices_A_list.append(edg_indices_A_offset)
                q_edgFeatures_H_list.append(q_edg_features_H)
                q_edgIndices_H_list.append(q_edg_indices_H_offset)
                labels_list.append(labels_data)

                # Sonraki grafik iÃ§in ofsetleri gÃ¼ncelle
                var_node_offset += var_features.shape[0]
                con_node_offset += con_features.shape[0]
                
            except Exception as e:
                if args.verbose:
                    print(f"âš ï¸  Data_{i} yÃ¼klenemedi: {e}. Bu Ã¶rnek atlandÄ±.")
                continue # Hata durumunda bu Ã¶rneÄŸi atla

        # TÃ¼m listelenen verileri tek NumPy dizilerinde birleÅŸtir
        varFeatures_all = np.vstack(varFeatures_list)
        conFeatures_all = np.vstack(conFeatures_list)
        edgFeatures_A_all = np.vstack(edgFeatures_A_list)
        edgIndices_A_all = np.vstack(edgIndices_A_list)
        q_edgFeatures_H_all = np.vstack(q_edgFeatures_H_list)
        q_edgIndices_H_all = np.vstack(q_edgIndices_H_list)
        labels_all = np.vstack(labels_list)

        # NumPy dizilerini TensorFlow tensÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼r ve batch verisini oluÅŸtur
        batch_data = (
            tf.constant(conFeatures_all, dtype=tf.float32),
            tf.transpose(tf.constant(edgIndices_A_all, dtype=tf.int32)), # Kenar indeksleri iÃ§in transpozisyon gerekli olabilir
            tf.constant(edgFeatures_A_all, dtype=tf.float32),
            tf.constant(varFeatures_all, dtype=tf.float32),
            tf.transpose(tf.constant(q_edgIndices_H_all, dtype=tf.int32)), # Kare kenar indeksleri iÃ§in transpozisyon gerekli olabilir
            tf.constant(q_edgFeatures_H_all, dtype=tf.float32),
            tf.constant(conFeatures_all.shape[0], dtype=tf.int32), # Toplam kÄ±sÄ±t dÃ¼ÄŸÃ¼mÃ¼ sayÄ±sÄ±
            tf.constant(varFeatures_all.shape[0], dtype=tf.int32), # Toplam deÄŸiÅŸken dÃ¼ÄŸÃ¼mÃ¼ sayÄ±sÄ±
            tf.constant(self.data_dimensions['n_cons'], dtype=tf.int32), # Her bir grafikteki kÄ±sÄ±t sayÄ±sÄ± (GNN katmanlarÄ± iÃ§in gerekli)
            tf.constant(self.data_dimensions['n_vars'], dtype=tf.int32), # Her bir grafikteki deÄŸiÅŸken sayÄ±sÄ± (GNN katmanlarÄ± iÃ§in gerekli)
            tf.constant(labels_all, dtype=tf.float32) # EÄŸitim etiketleri
        )
        
        return batch_data
    
    def create_model(self):
        """
        GNN modelini (QPGNNPolicy) belirtilen argÃ¼manlara gÃ¶re oluÅŸturur ve 
        TensorFlow optimizatÃ¶rÃ¼nÃ¼ (AdamW) baÅŸlatÄ±r.
        """
        output_units = 1
        output_activation = None # VarsayÄ±lan olarak aktivasyon yok (lineer Ã§Ä±ktÄ±)
        
        if args.type == "fea":
            output_activation = 'sigmoid' # Olabilirlik iÃ§in 0-1 aralÄ±ÄŸÄ±nda Ã§Ä±ktÄ±
        elif args.type == "sol":
            output_units = self.data_dimensions['n_vars'] # Ã‡Ã¶zÃ¼m vektÃ¶rÃ¼ iÃ§in deÄŸiÅŸken sayÄ±sÄ± kadar Ã§Ä±ktÄ±
            output_activation = None
        elif args.type == "obj":
            output_units = 1 # AmaÃ§ fonksiyonu iÃ§in tek bir skaler Ã§Ä±ktÄ±
            output_activation = None # Z-skor normalizasyonu sonrasÄ± genellikle lineer Ã§Ä±ktÄ± kullanÄ±lÄ±r
        
        self.model = QPGNNPolicy(
            emb_size=args.emb_size,
            cons_nfeats=self.data_dimensions['con_features'],
            edge_nfeats=self.data_dimensions['edge_features'],
            var_nfeats=self.data_dimensions['var_features'],
            qedge_nfeats=self.data_dimensions['qedge_features'],
            is_graph_level=(args.type != "sol"), # 'sol' tipi dÃ¼ÄŸÃ¼m seviyesinde, diÄŸerleri grafik seviyesinde tahmin yapar
            output_units=output_units,
            output_activation=output_activation,
            dropout_rate=args.dropout
        )
        
        self.optimizer = tf.keras.optimizers.AdamW(
            learning_rate=0.001,  # Ã–ÄŸrenme oranÄ±, LR yÃ¶neticisi tarafÄ±ndan Ã¼zerine yazÄ±lacaktÄ±r
            weight_decay=args.weight_decay
        )
        
        self.lr_manager = AdaptiveLRManager(
            self.optimizer,
            schedule_type=args.lr_schedule,
            total_epochs=args.epochs
        )
        
        print(f"ğŸ”§ Model baÅŸarÄ±yla oluÅŸturuldu:")
        print(f"   Tipi: {args.type}")
        print(f"   GÃ¶mme boyutu: {args.emb_size}")
        print(f"   Ã‡Ä±kÄ±ÅŸ birimleri: {output_units}")
        print(f"   Dropout oranÄ±: {args.dropout}")
        print(f"   LR Ã‡izelgesi: {args.lr_schedule}")
        print(f"   AÄŸÄ±rlÄ±k dÃ¼ÅŸÃ¼ÅŸÃ¼: {args.weight_decay}")
    
    def compute_loss(self, y_true, y_pred):
        """
        Model tÃ¼rÃ¼ne gÃ¶re uygun kayÄ±p fonksiyonunu hesaplar.
        'obj' iÃ§in baÄŸÄ±l mutlak hata, 'sol' ve 'fea' iÃ§in ortalama kare hata kullanÄ±lÄ±r.
        """
        if args.type == "obj":
            # AmaÃ§ fonksiyonu iÃ§in baÄŸÄ±l mutlak hata (relative absolute error)
            # Normalize edilmiÅŸ deÄŸerler iÃ§in de etkili olabilir.
            epsilon = 1e-6 # SÄ±fÄ±ra bÃ¶lmeyi Ã¶nlemek iÃ§in kÃ¼Ã§Ã¼k bir deÄŸer
            return tf.reduce_mean(tf.abs(y_true - y_pred) / (tf.abs(y_true) + epsilon))
            # Alternatif olarak, normalize edilmiÅŸ deÄŸerler iÃ§in doÄŸrudan MSE de iyi Ã§alÄ±ÅŸabilir:
            # return tf.reduce_mean(tf.square(y_true - y_pred))
        elif args.type == "sol":
            # Ã‡Ã¶zÃ¼m tahmini iÃ§in Ortalama Kare Hata (Mean Squared Error)
            return tf.reduce_mean(tf.square(y_true - y_pred))
        else:  # args.type == "fea" (Olabilirlik)
            # Olabilirlik tahmini iÃ§in Ortalama Kare Hata (Mean Squared Error)
            return tf.reduce_mean(tf.square(y_true - y_pred))
    
    @tf.function # TensorFlow grafiÄŸi olarak derlenerek performans artÄ±ÅŸÄ± saÄŸlar
    def train_step(self, batch_data):
        """Tek bir eÄŸitim adÄ±mÄ±nÄ± gerÃ§ekleÅŸtirir (ileri yayÄ±lÄ±m, kayÄ±p hesaplama, geri yayÄ±lÄ±m, aÄŸÄ±rlÄ±k gÃ¼ncelleme)."""
        *batched_states, labels = batch_data # Batch verilerini ve etiketleri ayÄ±r
        
        with tf.GradientTape() as tape: # GradyanlarÄ± kaydetmek iÃ§in GradientTape kullan
            predictions = self.model(batched_states, training=True) # Modeli eÄŸitim modunda Ã§alÄ±ÅŸtÄ±r
            loss = self.compute_loss(labels, predictions) # Tahminler ve gerÃ§ek etiketler arasÄ±ndaki kaybÄ± hesapla
        
        gradients = tape.gradient(loss, self.model.trainable_variables) # Modelin eÄŸitilebilir deÄŸiÅŸkenleri iÃ§in gradyanlarÄ± hesapla
        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables)) # GradyanlarÄ± uygulayarak model aÄŸÄ±rlÄ±klarÄ±nÄ± gÃ¼ncelle
        
        return loss # Hesaplanan kaybÄ± dÃ¶ndÃ¼r
    
    @tf.function # TensorFlow grafiÄŸi olarak derlenerek performans artÄ±ÅŸÄ± saÄŸlar
    def val_step(self, batch_data):
        """Tek bir doÄŸrulama adÄ±mÄ±nÄ± gerÃ§ekleÅŸtirir (ileri yayÄ±lÄ±m, kayÄ±p hesaplama)."""
        *batched_states, labels = batch_data
        
        predictions = self.model(batched_states, training=False) # Modeli Ã§Ä±karÄ±m (deÄŸerlendirme) modunda Ã§alÄ±ÅŸtÄ±r
        loss = self.compute_loss(labels, predictions) # KaybÄ± hesapla
        
        return loss # Hesaplanan kaybÄ± dÃ¶ndÃ¼r
    
    def train(self):
        """Modelin ana eÄŸitim dÃ¶ngÃ¼sÃ¼nÃ¼ yÃ¶netir."""
        print("ğŸ“‚ EÄŸitim verileri yÃ¼kleniyor...")
        train_data = self.load_batch_data(self.train_indices)
        
        print("ğŸ“‚ DoÄŸrulama verileri yÃ¼kleniyor...")
        val_data = self.load_batch_data(self.val_indices)
        
        best_val_loss = float('inf') # Åimdiye kadarki en iyi doÄŸrulama kaybÄ±nÄ± tutar
        best_epoch = 0              # En iyi kaybÄ±n elde edildiÄŸi epoch'u tutar
        model_path = f'./saved-models/qp_{args.type}_s{args.emb_size}.pkl' # Modelin kaydedileceÄŸi dosya yolu
        
        print(f"\nğŸš€ EÄŸitim baÅŸladÄ±!")
        print(f"Model: {args.type} | Epochlar: {args.epochs} | Erken Durma Yok (Best Model Saved)")
        print("-" * 80)
        
        for epoch in range(args.epochs):
            # EÄŸitim ve doÄŸrulama adÄ±mlarÄ±nÄ± Ã§alÄ±ÅŸtÄ±r
            train_loss = self.train_step(train_data).numpy() # TensorFlow tensÃ¶rÃ¼nÃ¼ NumPy deÄŸerine dÃ¶nÃ¼ÅŸtÃ¼r
            val_loss = self.val_step(val_data).numpy()       # TensorFlow tensÃ¶rÃ¼nÃ¼ NumPy deÄŸerine dÃ¶nÃ¼ÅŸtÃ¼r
            
            # Adaptif Ã¶ÄŸrenme oranÄ± yÃ¶neticisini gÃ¼ncelle
            lr_changed = self.lr_manager.step(epoch, val_loss)
            current_lr = self.optimizer.learning_rate.numpy()
            
            # EÄŸitim ilerlemesini konsola yazdÄ±r
            # AmaÃ§ fonksiyonu iÃ§in doÄŸrulama kaybÄ±nÄ±n normalleÅŸtirilmiÅŸ olduÄŸunu belirtiyoruz
            print(f"Epoch {epoch:4d}: EÄŸitim KaybÄ±={train_loss:.6f}, DoÄŸrulama KaybÄ± (Normalize EdilmiÅŸ)={val_loss:.6f}, LR={current_lr:.2e}")
            
            # EÄŸer mevcut doÄŸrulama kaybÄ± ÅŸimdiye kadarki en iyiyse modeli kaydet
            # Not: Erken durma (early stopping) uygulanmadÄ±ÄŸÄ± iÃ§in eÄŸitim tÃ¼m epoch'larÄ± tamamlayacak,
            # ancak en iyi performans gÃ¶steren model kaydedilecektir.
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                best_epoch = epoch
                self.model.save_state(model_path) # Modelin aÄŸÄ±rlÄ±klarÄ±nÄ± ve durumunu kaydeder
                print(f"  âœ“ En iyi model kaydedildi (doÄŸrulama_kaybÄ±={val_loss:.6f})")
        
        print("-" * 80)
        print(f"ğŸ‰ EÄŸitim tamamlandÄ±!")
        print(f"En iyi doÄŸrulama kaybÄ± (normalize edilmiÅŸ): {best_val_loss:.6f} (epoch {best_epoch})")
        print(f"Final model kaydedildi: {model_path}")
    
    def run(self):
        """TÃ¼m eÄŸitim sÃ¼recini baÅŸlatan ana fonksiyondur."""
        print("ğŸš€ QP GNN Model EÄŸitimi BaÅŸlatÄ±lÄ±yor...")
        print("=" * 60)
        
        # GPU'yu ayarla veya CPU'ya dÃ¼ÅŸ
        device = self.setup_gpu()
        # Veri boyutlarÄ±nÄ± algÄ±la (modelin baÅŸlatÄ±lmasÄ± iÃ§in gerekli)
        self.detect_data_dimensions()
        # EÄŸitim ve doÄŸrulama ayrÄ±mÄ±nÄ± oluÅŸtur ve 'obj' iÃ§in normalizasyon istatistiklerini hesapla
        self.create_train_val_split()
        
        # Belirtilen cihazda (GPU veya CPU) model oluÅŸturma ve eÄŸitimi baÅŸlat
        with tf.device(device):
            self.create_model()
            
            # Modelleri kaydetmek iÃ§in dizin oluÅŸtur
            os.makedirs('./saved-models', exist_ok=True)
            
            # EÄŸitimi baÅŸlat
            self.train()

if __name__ == "__main__":
    trainer = QPModelTrainer()
    trainer.run()